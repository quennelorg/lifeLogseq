# Data Milky Way: A Brief History (Part 1) - OLTP vs. OLAP
collapsed:: true
	- Data Engineering: The Good Parts (1/4)
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=8VD2qoebacA&t=8s&ab_channel=PawaritLaosunthara}}
	- ## Basic Definitions
	  collapsed:: true
		- BIG DATA
		- ![data-milky-way-d541a2663522fc192c791bef52f722ce.png](../assets/data-milky-way-d541a2663522fc192c791bef52f722ce_1718693609914_0.png)
		- Databases, NoSQL, Data Warehouses, Data Lakes, Sparkâ€¦and many other terms in this domain space!
		- You've probably come across at least a few of these terms, but you might be wondering what these technologies do and how do they fit together with one another in the vast landscape of Data Engineering and AI.
	- ## OLTP vs OLAP
	  collapsed:: true
		- ### OLTP (åœ¨çº¿äº‹åŠ¡å¤„ç†)
			- **ç”¨é€”**ï¼šOLTPç³»ç»Ÿä¸»è¦ç”¨äºŽæ—¥å¸¸äº‹åŠ¡å¤„ç†ï¼Œé€‚ç”¨äºŽé«˜é¢‘çŽ‡ã€å°è§„æ¨¡çš„äº¤æ˜“æ“ä½œã€‚å…¸åž‹åº”ç”¨åŒ…æ‹¬é“¶è¡Œäº¤æ˜“ã€è®¢å•å¤„ç†å’Œåº“å­˜ç®¡ç†ç­‰ã€‚å…¶ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬å¤„ç†æ—¥å¸¸äº‹åŠ¡å’Œæ“ä½œï¼Œä¾‹å¦‚è®¢å•å¤„ç†ã€é“¶è¡Œäº¤æ˜“ã€å®¢æˆ·ç®¡ç†ç³»ç»Ÿç­‰ã€‚
			- **ç‰¹å¾**ï¼š
				- **å®žæ—¶æ€§**ï¼šæ”¯æŒå¿«é€Ÿã€åŠæ—¶çš„äº‹åŠ¡å¤„ç†ã€‚
				- **æ•°æ®æ›´æ–°é¢‘ç¹**ï¼šæ¶‰åŠå¤§é‡çš„æ’å…¥ã€æ›´æ–°å’Œåˆ é™¤æ“ä½œã€‚
				- **æ•°æ®é‡å°ä½†é¢‘ç¹**ï¼šæ¯ä¸ªäº‹åŠ¡æ¶‰åŠçš„æ•°æ®é‡è¾ƒå°ï¼Œä½†äº‹åŠ¡æ•°é‡å¤šã€‚
				- **é«˜å¹¶å‘æ€§**ï¼šæ”¯æŒå¤§é‡ç”¨æˆ·åŒæ—¶è¿›è¡Œè¯»å†™æ“ä½œã€‚
				- **å®žæ—¶å¤„ç†**ï¼šäº¤æ˜“éœ€è¦ç«‹å³å¤„ç†å’Œè®°å½•ã€‚
				- **æ•°æ®ä¸€è‡´æ€§**ï¼šç¡®ä¿äº‹åŠ¡çš„åŽŸå­æ€§ã€ä¸€è‡´æ€§ã€éš”ç¦»æ€§å’ŒæŒä¹…æ€§ï¼ˆACIDç‰¹æ€§ï¼‰ã€‚
				- **å°è§„æ¨¡æŸ¥è¯¢**ï¼šæŸ¥è¯¢é€šå¸¸æ¶‰åŠå°‘é‡è®°å½•å’Œç®€å•çš„æ•°æ®åº“æ“ä½œã€‚
			- Typical use cases and implementations:
				- Application databases
				- Caches
				- SQL and NoSQL ("Not Only SQL") - lots of diverse technologies in the OLTP space.
		- ### OLAP (åœ¨çº¿åˆ†æžå¤„ç†)
			- **ç”¨é€”**ï¼šè¿›è¡Œå¤æ‚çš„æŸ¥è¯¢å’Œæ•°æ®åˆ†æžï¼Œä¾‹å¦‚å•†ä¸šæ™ºèƒ½ã€æŠ¥è¡¨ç”Ÿæˆã€æ•°æ®æŒ–æŽ˜ç­‰ã€‚
			- **ç‰¹å¾**ï¼š
				- **å¤æ‚æŸ¥è¯¢**ï¼šæ”¯æŒå¤æ‚çš„æŸ¥è¯¢å’Œæ•°æ®èšåˆæ“ä½œï¼Œé€šå¸¸æ¶‰åŠå¤§é‡æ•°æ®ã€‚
				- **æ•°æ®åŽ†å²æ€§**ï¼šå¤„ç†çš„æ˜¯é•¿æ—¶é—´ç´¯ç§¯çš„æ•°æ®ï¼Œä»¥ä¾¿è¿›è¡Œè¶‹åŠ¿åˆ†æžå’ŒåŽ†å²æ•°æ®æŒ–æŽ˜ã€‚
				- **ä½Žå¹¶å‘æ€§**ï¼šä¸ŽOLTPç›¸æ¯”ï¼ŒOLAPç³»ç»Ÿçš„ç”¨æˆ·å¹¶å‘æ•°è¾ƒä½Žï¼Œä½†æ¯ä¸ªæŸ¥è¯¢çš„å¤æ‚åº¦è¾ƒé«˜ã€‚
				- **å¤§è§„æ¨¡æ•°æ®æ“ä½œ**ï¼šæŸ¥è¯¢é€šå¸¸æ¶‰åŠå¤§é‡è®°å½•ï¼Œéœ€è¦é«˜æ•ˆçš„æ•°æ®è¯»å–å’Œåˆ†æžèƒ½åŠ›ã€‚
			- Typical use cases:
				- **Data Warehouses**: A way to implement data models in a database to cater to OLAP style workloads.
				- **Data Lakes**: Have emerged strongly within the last 5-10 years to more exclusively match the requirements of large data workloads.
		- ### åŒºåˆ«æ€»ç»“
			- 1.  **æ“ä½œç±»åž‹**ï¼šOLTPä¾§é‡äºŽæ’å…¥ã€æ›´æ–°å’Œåˆ é™¤æ“ä½œï¼ŒOLAPä¾§é‡äºŽå¤æ‚æŸ¥è¯¢å’Œåˆ†æžã€‚
			- 2.  **æ•°æ®é‡**ï¼šOLTPå¤„ç†è¾ƒå°çš„æ•°æ®é‡ä½†æ“ä½œé¢‘ç¹ï¼ŒOLAPå¤„ç†å¤§æ•°æ®é‡ä¸”ä¸»è¦æ˜¯è¯»å–æ“ä½œã€‚
			- 3.  **å“åº”æ—¶é—´**ï¼šOLTPéœ€è¦å¿«é€Ÿå“åº”ä»¥æ”¯æŒå®žæ—¶äº‹åŠ¡å¤„ç†ï¼ŒOLAPå…è®¸è¾ƒé•¿çš„æŸ¥è¯¢æ—¶é—´ä»¥æ”¯æŒæ·±å…¥åˆ†æžã€‚
			- 4. **ç›®æ ‡**ï¼šOLTPæ—¨åœ¨æ”¯æŒæ—¥å¸¸ä¸šåŠ¡æ“ä½œï¼Œè€ŒOLAPæ—¨åœ¨æ”¯æŒä¸šåŠ¡å†³ç­–å’Œæ•°æ®åˆ†æžã€‚
			- 5. **æ•°æ®å¤„ç†**ï¼šOLTPå¤„ç†ç®€å•å’Œé¢‘ç¹çš„äº‹åŠ¡æ€§æ•°æ®ï¼Œè€ŒOLAPå¤„ç†å¤æ‚å’Œä½Žé¢‘çŽ‡çš„åˆ†æžæ€§æ•°æ®ã€‚
			- 6. **æ•°æ®åº“è®¾è®¡**ï¼šOLTPæ•°æ®åº“è®¾è®¡å¼ºè°ƒè§„èŒƒåŒ–ï¼Œä»¥å‡å°‘å†—ä½™å’Œæé«˜æ•°æ®ä¸€è‡´æ€§ï¼›OLAPæ•°æ®åº“è®¾è®¡åˆ™é€šå¸¸é‡‡ç”¨ç»´åº¦å»ºæ¨¡ï¼Œå¦‚æ˜Ÿåž‹æˆ–é›ªèŠ±åž‹æž¶æž„ï¼Œä»¥ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ã€‚
		- ### å®žä¾‹åº”ç”¨
			- **OLTP**ï¼šé“¶è¡Œçš„äº¤æ˜“å¤„ç†ç³»ç»Ÿã€ç”µå­å•†åŠ¡å¹³å°çš„è®¢å•ç®¡ç†ç³»ç»Ÿã€‚
			- **OLAP**ï¼šä¼ä¸šçš„æ•°æ®ä»“åº“ã€æŠ¥è¡¨å’Œå•†ä¸šæ™ºèƒ½ç³»ç»Ÿã€‚
		- # SQL Tutorial: OLTP and OLAP
			- {{video https://www.youtube.com/watch?v=-v3PhEtOuxw&ab_channel=DataCamp}}
	- ## From Database to Data Lake[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-1#from-database-to-data-lake "Direct link to From Database to Data Lake")
	  collapsed:: true
		- ### Step 1: Database to Data Warehouse
			- **Watch:**Â Database vs. Data Warehouse
				- {{video https://www.youtube.com/watch?v=FxQG65OhXAQ&t=1s&ab_channel=intricity101}}
			- **Read:**Â [The Difference Between a Database and a Data Warehouse](https://panoply.io/data-warehouse-guide/the-difference-between-a-database-and-a-data-warehouse/)
		- ### Step 2: Data Warehouse to Data Lake
			- Let's take it one step further and have a look at the more commonly found implementation today, the Data Lake.
			- **Read:**Â [Data Warehouse vs Data Lake](https://panoply.io/data-warehouse-guide/data-warehouse-vs-data-lake/)
	- ## Focus of this course: OLAP workloads[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-1#focus-of-this-course-olap-workloads "Direct link to Focus of this course: OLAP workloads")
	  collapsed:: true
		- **Why are we focusing more on OLAP than OLTP?**
			- OLTP databases (sometimes interchangeably referred to as application databases) are a topic concerning stability and performance of your operations / live applications
			- OLTP data models and tech stacks for different problems/businesses vary a lot more than those for OLAP workloads!
			- When people/customers talk about developing Data & AI, Analytics, Data Science, Machine Learning, theyâ€™re most likely referring to OLAP-style workloads.
-
- # Data Milky Way: Distributed Data Systems (NoSQL & CAP)
  collapsed:: true
	- ## Introduction to NoSQL â€¢ MartinFowler â€¢ GOTO 2012
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=qI_g07C_Q5I&t=484s&ab_channel=GOTOConferences}}
	- ## CAP Theorem
	  collapsed:: true
		- Lesson 111 - CAP Theorem Illustrated
		  collapsed:: true
			- {{video https://www.youtube.com/watch?v=9SSvdLnmDiI&ab_channel=MarkRichards}}
		- CAPå®šç†ï¼ˆCAP Theoremï¼‰ï¼Œä¹Ÿè¢«ç§°ä¸ºå¸ƒé²å°”å®šç†ï¼ˆBrewer's Theoremï¼‰ï¼Œæ˜¯åˆ†å¸ƒå¼è®¡ç®—ä¸­çš„ä¸€ä¸ªé‡è¦ç†è®ºï¼Œç”±è®¡ç®—æœºç§‘å­¦å®¶Eric Breweråœ¨2000å¹´æå‡ºï¼Œå¹¶åœ¨2002å¹´ç”±Seth Gilbertå’ŒNancy Lynchæ­£å¼è¯æ˜Žã€‚CAPå®šç†è¯´æ˜Žäº†åœ¨åˆ†å¸ƒå¼æ•°æ®å­˜å‚¨ç³»ç»Ÿä¸­ï¼Œæœ‰ä¸‰ç§é‡è¦çš„ç‰¹æ€§ï¼Œä½†åªèƒ½åŒæ—¶æ»¡è¶³å…¶ä¸­çš„ä¸¤ç§ï¼š
			- 1.  **ä¸€è‡´æ€§ï¼ˆConsistencyï¼‰**ï¼š
				- ç³»ç»Ÿåœ¨è¯»å–è¯·æ±‚æ—¶ï¼Œå§‹ç»ˆèƒ½å¤Ÿè¿”å›žæœ€æ–°çš„å†™å…¥ç»“æžœã€‚è¿™æ„å‘³ç€æ‰€æœ‰èŠ‚ç‚¹åœ¨ä»»ä½•æ—¶åˆ»éƒ½èƒ½çœ‹åˆ°ç›¸åŒçš„æ•°æ®ã€‚
			- 2.  **å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰**ï¼š
				- æ¯ä¸ªè¯·æ±‚ï¼ˆæ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼‰éƒ½èƒ½æ”¶åˆ°éžé”™è¯¯å“åº”ã€‚æ¢å¥è¯è¯´ï¼Œç³»ç»Ÿåœ¨ä»»ä½•æ—¶å€™éƒ½èƒ½æä¾›æœåŠ¡ã€‚
			- 3.  **åˆ†åŒºå®¹å¿æ€§ï¼ˆPartition Toleranceï¼‰**ï¼š
				- ç³»ç»Ÿèƒ½å¤Ÿç»§ç»­æ“ä½œï¼Œå³ä½¿å­˜åœ¨ç½‘ç»œåˆ†åŒºï¼ˆç½‘ç»œæ•…éšœå¯¼è‡´èŠ‚ç‚¹ä¹‹é—´æ— æ³•é€šä¿¡ï¼‰ã€‚è¿™æ„å‘³ç€ç³»ç»Ÿåœ¨é¢å¯¹èŠ‚ç‚¹æˆ–ç½‘ç»œæ•…éšœæ—¶ä»ç„¶èƒ½ç»´æŒå…¶è¿ä½œã€‚
		- ### CAPå®šç†çš„é™åˆ¶
			- æ ¹æ®CAPå®šç†ï¼Œåœ¨ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œå½“å‡ºçŽ°ç½‘ç»œåˆ†åŒºæ—¶ï¼Œå¿…é¡»åœ¨ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚è¿™æ„å‘³ç€ï¼š
				- **CAï¼ˆä¸€è‡´æ€§ + å¯ç”¨æ€§ï¼‰**ï¼š åœ¨æ²¡æœ‰ç½‘ç»œåˆ†åŒºçš„æƒ…å†µä¸‹ï¼Œç³»ç»Ÿå¯ä»¥åŒæ—¶å…·å¤‡ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ã€‚ä½†ä¸€æ—¦å‡ºçŽ°ç½‘ç»œåˆ†åŒºï¼Œç³»ç»Ÿå°†æ— æ³•æ»¡è¶³åˆ†åŒºå®¹å¿æ€§ã€‚
				- **CPï¼ˆä¸€è‡´æ€§ + åˆ†åŒºå®¹å¿æ€§ï¼‰**ï¼š ç³»ç»Ÿåœ¨é¢å¯¹ç½‘ç»œåˆ†åŒºæ—¶ä»ç„¶èƒ½ä¿æŒä¸€è‡´æ€§ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²ä¸€äº›å¯ç”¨æ€§ï¼ˆå³éƒ¨åˆ†è¯·æ±‚å¯èƒ½ä¼šå¤±è´¥æˆ–å»¶è¿Ÿï¼‰ã€‚
				- **APï¼ˆå¯ç”¨æ€§ + åˆ†åŒºå®¹å¿æ€§ï¼‰**ï¼š ç³»ç»Ÿåœ¨é¢å¯¹ç½‘ç»œåˆ†åŒºæ—¶ä»ç„¶èƒ½ä¿æŒé«˜å¯ç”¨æ€§ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²ä¸€è‡´æ€§ï¼ˆå³ä¸åŒèŠ‚ç‚¹å¯èƒ½ä¼šè¿”å›žä¸åŒçš„æ•°æ®ï¼‰ã€‚
		- ### å®žé™…åº”ç”¨ä¸­çš„é€‰æ‹©
			- åœ¨è®¾è®¡åˆ†å¸ƒå¼ç³»ç»Ÿæ—¶ï¼Œå·¥ç¨‹å¸ˆå¿…é¡»æ ¹æ®å®žé™…éœ€æ±‚åœ¨ä¸€è‡´æ€§ã€å¯ç”¨æ€§å’Œåˆ†åŒºå®¹å¿æ€§ä¹‹é—´è¿›è¡Œé€‰æ‹©ã€‚ä¾‹å¦‚ï¼š
				- **ä¸€è‡´æ€§ä¼˜å…ˆ**ï¼šé‡‘èžäº¤æ˜“ç³»ç»Ÿé€šå¸¸éœ€è¦é«˜ä¸€è‡´æ€§ï¼Œä»¥ç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚
				- **å¯ç”¨æ€§ä¼˜å…ˆ**ï¼šç¤¾äº¤ç½‘ç»œç­‰åº”ç”¨é€šå¸¸éœ€è¦é«˜å¯ç”¨æ€§ï¼Œä»¥ç¡®ä¿ç”¨æˆ·åœ¨ä»»ä½•æ—¶å€™éƒ½èƒ½è®¿é—®æœåŠ¡ï¼Œå³ä½¿è¿™å¯èƒ½å¯¼è‡´çŸ­æ—¶é—´çš„æ•°æ®ä¸ä¸€è‡´ã€‚
				- **åˆ†åŒºå®¹å¿æ€§ä¼˜å…ˆ**ï¼šå¤§å¤šæ•°çŽ°ä»£åˆ†å¸ƒå¼ç³»ç»Ÿéƒ½å¿…é¡»å¤„ç†ç½‘ç»œåˆ†åŒºï¼Œå› æ­¤åˆ†åŒºå®¹å¿æ€§æ˜¯ä¸€ä¸ªåŸºæœ¬è¦æ±‚ã€‚
		- ### ç»“è®º
			- CAPå®šç†å¼ºè°ƒäº†åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡ä¸­çš„æƒè¡¡å’ŒæŒ‘æˆ˜ï¼Œæé†’æˆ‘ä»¬åœ¨è¿½æ±‚é«˜æ€§èƒ½å’Œé«˜å¯é æ€§çš„åŒæ—¶ï¼Œå¿…é¡»åœ¨ä¸€è‡´æ€§ã€å¯ç”¨æ€§å’Œåˆ†åŒºå®¹å¿æ€§ä¹‹é—´åšå‡ºå–èˆâ€‹ ([Tech Differences](https://techdifferences.com/difference-between-data-mining-and-data-warehousing.html))â€‹â€‹ ([Tech Differences](https://techdifferences.com/difference-between-server-and-database.html))â€‹ã€‚
		- **Summary: Pick Two:**
			- **C**Â (consistency)
			- **A**Â (availability)
			- **P**Â (partition tolerance)
		- But is it really true in the real world?
			- What unrealistic assumption are we making here?
				- Can we really assume that network communications wonâ€™t fail?
				- Is there really such thing as a distributed system that wonâ€™t have partitions?
			- In Reality: not a binary choice between C and A
			- Several NoSQL solutions offer a tunable tradeoff between C and A
		- **Further Reading (bonus content):**Â [What is the CAP Theorem? 3 types of NoSQL databases](https://www.ibm.com/cloud/learn/cap-theorem#toc-the-cap-in-9y9p3uen)
	- ## Summary[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-nosql-cap-theorem#summary "Direct link to Summary")
	  collapsed:: true
		- A CP system will say â€œSorry, I canâ€™t be sure yetâ€ to the client, in order to avoid giving an out-of-date answer.
		- ![cap-82a836f9e5a694080bdb9bdaa6418567.png](../assets/cap-82a836f9e5a694080bdb9bdaa6418567_1718693542097_0.png)
		- An AP system tries to spits out an answer even if it might not be the most up-to-date one.
		- ![cap-wikipedia.png](https://data-derp.github.io/assets/images/cap-wikipedia-8882aabb817ad1eae3f31aa32f2cc713.png)
		- FromÂ [Wikipedia](https://en.wikipedia.org/wiki/CAP_theorem)
-
- # Data Milky Way: A Brief History (Part 2) - Evolution
  collapsed:: true
	- Data Engineering: The Good Parts (2/4)
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=MxSwJQvWKuw&t=3s&ab_channel=PawaritLaosunthara}}
	-
	- ## 1980s: Databases & Data Warehouses[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#1980s-databases--data-warehouses "Direct link to 1980s: Databases & Data Warehouses")
	  collapsed:: true
		- Since the 1980s up until the mid-late 2000s, businesses still stored bothÂ [OLTP and OLAP](https://www.youtube.com/watch?v=-v3PhEtOuxw&ab_channel=DataCamp)Â data in relational databases (RDBMS)
			- e.g. Oracle, Microsoft SQL Server, MySQL, PostgreSQL, etc.
			- OLAP-oriented databases for storing large amounts of data were calledÂ **Data Warehouses**
			- Relational databases were notoriously difficult to distribute/scale out
			- As more data came in, businesses often opted to scale up ðŸ’° ([scale up vs scale out](https://hackernoon.com/database-scaling-horizontal-and-vertical-scaling-85edd2fd9944))
				- Greater investment risk, difficult to plan capacity, expensive
				- Restrictive limits to CPU & memory for a single server
				- Nowadays, modern Massively Parallel Processing (MPP) warehouses can provide scale out capabilities
		- However, Data Warehouses still require carefulÂ **upfront**Â planning
			- Schema and layout need to be decided beforehand
			- Query patterns/use-cases needed to be preempted
			- Separation of storage and compute often a weakness (perhaps except for modern warehouses such as Google BigQuery and Snowflake)
		- ![scaling-c4dfb525eeab8f891d88a0458f1cb95c.png](../assets/scaling-c4dfb525eeab8f891d88a0458f1cb95c_1718693670784_0.png)
	- ## Data Warehouses[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#data-warehouses "Direct link to Data Warehouses")
	  collapsed:: true
		- ![data-warehouses-dd7dbd350cc72c69036ec01a13c40333.png](../assets/data-warehouses-dd7dbd350cc72c69036ec01a13c40333_1718693682788_0.png)
		- Pros
			- Good for Business Intelligence (BI) applications (structured data, so long as it isn't too massive)
		- Cons
			- **Limited support**Â for advanced analytics & machine learning workloads
			- **Limited support**Â for non-tabular, unstructured data (e.g. free text, images)
			- Proprietary systems with only a SQL interface
		- Because of the limitations around flexibility and scaling, a new technology emerged in the early 2000s.
	- ## Early 2000s: Hadoop Hype Train[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#early-2000s-hadoop-hype-train "Direct link to Early 2000s: Hadoop Hype Train")
	  collapsed:: true
		- Arrival of On-Prem Data Lakes (2004) Google published theÂ [MapReduce](https://www.youtube.com/watch?v=s8EPQpgpWVE&ab_channel=CBTNuggets)Â whitepaper, inspiring the Apache Hadoop project
			- enabledÂ **(on-prem)**Â distributed data processing onÂ **commodity (cheap) hardware**
			- businesses began throwing data into their Hadoop clusters!
		- ![joy-of-hadoop-fab8097c1a5d35557360af1d76a3ae05.png](../assets/joy-of-hadoop-fab8097c1a5d35557360af1d76a3ae05_1718693745968_0.png)
		- ![hadoop-network-17aaeb45b6fb6410770b32f029fdc536.png](../assets/hadoop-network-17aaeb45b6fb6410770b32f029fdc536_1718693755098_0.png)
		- **Bonus content**:Â [What you need to know about Hadoop](https://www.oreilly.com/content/hadoop-what-you-need-to-know/)Â (Very detailed and not required for the remainder of the course!)
	- ## The Hadoop Disillusionment[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#the-hadoop-disillusionment "Direct link to The Hadoop Disillusionment")
	  collapsed:: true
		- Several years later, people becameÂ [disillusioned about Hadoop](https://www.datanami.com/2017/03/13/hadoop-failed-us-tech-experts-say/#:~:text=The%20Hadoop%20dream%20of%20unifying,find%20a%20happy%20Hadoop%20customer.):
		- ![hadoop-has-failed-us-8d5ef8a6cadb6f732caab6366c96b51e.png](../assets/hadoop-has-failed-us-8d5ef8a6cadb6f732caab6366c96b51e_1718693765084_0.png)
		- Working with distributed computing can be extremely challenging with respect to the learning curve, especially for business analysts. Instead of working with SQL, they'd have to learn Java libraries and frameworks related to MapReduce.
		- Excerpt:
			- INFO
				- Hadoop is great if you're a data scientist who knows how to code in MapReduce or Pig, Johnson says, but as you go higher up the stack, theÂ **abstraction layers have mostly failed to deliver on the promise of enabling business analystics to get at the data.**
		- In addition to the learning curve, the performance was surprisingly slow for BI users. They would have expected that compared to the warehouse, they would have had performance gains but in fact the queries were running a lot slower on hadoop than on the traditional data warehouses.
		- Excerpt:
			- INFO
				- "At the Hive layer, it's kind of OK. But people think they're going to use Hadoop for data warehouse...**are pretty surprised that this hot new technology is 10x slower than what they're using before**," Johnson says.
		- This disillusionment led to the sunsetting of Hadoop projects, including at Google (who were the inventors of the entire ecossystem).
		- ![urs-hoelzle-ad8f6a492b8454d933efa0b98d68e034.png](../assets/urs-hoelzle-ad8f6a492b8454d933efa0b98d68e034_1718693775690_0.png)
		- Senior Vice President of Technical Infrastructure, Google
		- While data locality + coupling storage and compute in Hadoop clusters was a decent idea for data throughputâ€¦
			- Businesses were forced to increaseÂ **both**Â CPU & Disk when they often only needed to scale up just one or the other
			- Youâ€™d have to pay for more CPU just to store inactive, rarely-utilized data, what a waste!
			- Storing and replicating data on HDFS (Hadoop Distributed File System) wasÂ **expensive**Â and difficult to maintain
			- Query performance wasÂ **lackluster**Â and other beneficial properties of RDBMS were gone
	- ## Latency[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#latency "Direct link to Latency")
	  collapsed:: true
		- Latency nightmare when only using disk and network (i.e. MapReduce). Let's get a feel for it relative to "humanized" time.
		- | Type | Time | Time (humanized form) |
		  | :-: | :-: | :-: |
		  | 1 CPU Cycle | 0.3 ns | 1 s |
		  | Level 1 Cache Access | 0.9 ns | 3 s |
		  | Level 2 Cache Access | 2.8 ns | 9 s |
		  | Level 3 Cache Access | 12.9 ns | 43 s |
		  | Main Memory Access | 120 ns | 6 min |
		  | Solid-state disk I/O | 50 - 150 Âµs | 2 - 6 days |
		  | Rotational disk I/O | 1 - 10 ms | 1 - 12 months |
		  | Internet: SF to NYC | 40 ms | 4 years |
		  | Internet: SF to Australia | 183 ms | 19 years |
		  | OS Virtualization reboot | 4 s | 23 years |
		  | SCSI command time-out | 30 s | 3000 years |
		  | Hardware Virtualization reboot | 40 s | 4000 years |
		  | Physical System reboot | 5 m | 32 millenia |
	- ## Latency matters in Hadoop[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#latency-matters-in-hadoop "Direct link to Latency matters in Hadoop")
	  collapsed:: true
		- Hadoop is all about fault tolerance and simple api of map and reduce steps. But all that fault tolerance in Hadoop comes at a very huge cost:
			- 1.  To recover from potential failures, Hadoop MapReduce writes intermediate data between Map and Reduce steps to the disk for every job.
			- 2.  The MapReduce implementation in Hadoop, by design, shuffles the data between map and reduce steps, which is expensive.
		- Now looking at the above table, we know that writing to the disk and network communications are slow and expensive and quite naturally increases latency. Because of the inherent behavior of MapReduce (constant data transmissions and persisting), attention must be paid to the amount of data that gets transmitted to manage both speed and costs of queries.
	- ## Spark is pre-built to reduce latency[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#spark-is-pre-built-to-reduce-latency "Direct link to Spark is pre-built to reduce latency")
	  collapsed:: true
		- Spark also provides fault tolerance like Hadoop but with significantly reduced latency in the following way :
			- 1.  It keeps all data immutable and in memory, thus avoiding disk writes, by keeping data in memory.
			- 2.  It provides Scala like chains of operations (called transformations) and keeps track of it in its light weight client side process called as Spark Driver.
			- 3.  In the event of a node failure, it finds a new node, copies the partition data on that node and just simply plays the tracked operations in step 2 above.
		- Additionally Spark provides rich APIs in Scala, Python, Java and R. Lastly, unlike hadoop, it makes data shuffle optional by making it necessary only when using certain spark operations (formally called wide transformations in Spark) like joins, groupBy etc.
		- **Optional content**: 24 minute video version of Latency in Big Data (demonstrates the massive impact of using memory vs disk vs network in Big Data):
		- [Big Data Analysis with Scala and Spark](https://www.youtube.com/@bigdataanalysiswithscalaan7712)
		  collapsed:: true
			- {{video https://www.youtube.com/watch?v=DXq5MOYGK1U&ab_channel=BigDataAnalysiswithScalaandSpark}}
	- ## Cloud Revolution[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#cloud-revolution "Direct link to Cloud Revolution")
	  collapsed:: true
		- Cloud Revolution:Â [Why object storage wins over Hadoop-based storage](https://www.ibm.com/cloud/blog/cutting-cord-separating-data-compute-data-lake-object-storage)
			- To scale cost-effectively, we need to really separate compute and storage
				- e.g. simply provision more CPU-intensive clusters only when needed, while leaving storage the same
			- As analytics and AI began to involve images, audio, unstructured data:
				- Cloud Data Lakes (often based on object storage) became the ideal storage solution
		- Time forÂ **unified analytics/query engines**Â such asÂ **Spark**Â andÂ **Presto**Â to shine ðŸ’«
			- [Spark](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch01.html)Â &Â [Presto](https://prestodb.io/overview.html)
				- Both engines excel when running analytical queries against data stored on Object Storage (e.g. Amazon S3, Azure Blob Storage)
				- Both engines take advantage ofÂ **both memory and disk**Â (unlike Hadoop MapReduce which read/writes data via disk only)
		- **Spark**Â (much more on this later) isÂ **extremely popular for programmatic (Python/Scala/Java/R) use-cases**Â but can also support SQL queries.
		- **Presto**Â is a popular choice forÂ **ad-hoc interactive SQL queries**Â (e.g. AWS Athena is a serverless offering based on Presto).
		- Many companies (especially tech giants) can even often have both Spark and Presto/Athena in their stack:
		- ![presto-spark-netflix-platform.png](https://data-derp.github.io/assets/images/presto-spark-netflix-platform-8df9d0def428f4eb82a020aff1c3d051.png)
	- ## Unified Analytics Engines[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#unified-analytics-engines "Direct link to Unified Analytics Engines")
	  collapsed:: true
		- Both Spark and Presto can be consideredÂ **Unified Analytics Engines**, meaning they can cover most of your data needs from ingestion from different sources over transformation to visualisations. What's special is that this multi-purpose approach enables you to adjust the technologies to your needs (and the arrangement of your data) and not vice versa.
		- ![spark-bf4bd89b0dff234dab523146a9acc592.png](../assets/spark-bf4bd89b0dff234dab523146a9acc592_1718693807527_0.png)
		- ![presto-cluster-bd6a6df4b1a33fe13e8265a7367d3f61.png](../assets/presto-cluster-bd6a6df4b1a33fe13e8265a7367d3f61_1718693813340_0.png)
		- ### Mission Accomplished?[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#mission-accomplished "Direct link to Mission Accomplished?")
			- While being able to directly query your cloud object storage (e.g. S3, Azure Data Lake Storage) with big data engines such as Spark and Presto moved the field in a great direction, some missing pieces still remained. Let's have a look at the setup of traditional data lakes versus more versatile solutions.
	- ## 2000s - 2010s: Traditional Data Lakes[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#2000s---2010s-traditional-data-lakes "Direct link to 2000s - 2010s: Traditional Data Lakes")
	  collapsed:: true
		- ![traditional-data-lakes-11c4126077dba26d8c34337b39f1bb12.png](../assets/traditional-data-lakes-11c4126077dba26d8c34337b39f1bb12_1718693822843_0.png)
		- **Pros**
			- Extremely cheap and scalable
			- Open, arbitrary data formats and big ecosystem
			- Supports ML
		- **Cons**
			- Some BI workloads still not snappy enough
			- Complex data quality problems
			- No data management layer
			- Difficult for GDPR compliance
	- ## Can we get the best of both worlds?[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-2-evolution/?nav=false#can-we-get-the-best-of-both-worlds "Direct link to Can we get the best of both worlds?")
	  collapsed:: true
		- The goal in this space is to get rid of the current shortcomings listed above. We want to continue to use cheap and scalable computing resources and a wide ecosystem while being able to tackle more advanced problems, opening up data related problems to entire companies while maintaining appropriate data security and access policies.
		- One approach is the so-calledÂ **Data Lakehouse**Â which weâ€™ll revisit in the second week of the course.
		- {:height 277, :width 512}
		- Other challengers in this area areÂ [Delta Lake](https://delta.io/),Â [Dremio](https://www.dremio.com/)Â orÂ [Databricks Photon](https://databricks.com/product/photon). Weâ€™ll also cover how Lakehouse can serve as the technological foundation to support philosophies such as Data Mesh near the end of this course.
		- ![data-lakehouse-a1943b71285c4f3be9e3df0e3cf9838b.png](../assets/data-lakehouse-a1943b71285c4f3be9e3df0e3cf9838b_1718693832808_0.png)
-
- # Data Milky Way: Brief History (Part 3) - Data Processing
  collapsed:: true
	- Data Engineering: The Good Parts (3/4)
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=Uc-Wtem-lyw&ab_channel=PawaritLaosunthara}}
	-
	- ## Evolution of Data Processing[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-3-data-processing/?nav=false#evolution-of-data-processing "Direct link to Evolution of Data Processing")
		- ![map-reduce-processing-2397bafb5a04130d82688f98633d37d8.png](../assets/map-reduce-processing-2397bafb5a04130d82688f98633d37d8_1718693475535_0.png)
		- ![data-processing-vision-a59ff56df35005ec4085de683d25e1bc.png](../assets/data-processing-vision-a59ff56df35005ec4085de683d25e1bc_1718693495610_0.png)
		- Recap:Â [Why move from Hadoop to Spark + Object Storage](https://www.xplenty.com/blog/apache-spark-vs-hadoop-mapreduce/)Â (~20 minute read)
		- **Bonus Content, not necessary for the progression in this course**:
			- [Intro to Metastores and Data Catalogs](https://lakefs.io/metadata-management-hive-metastore-vs-aws-glue/)Â (~10 minutes)
			- [Batch and Micro-Batch Streaming](https://www.upsolver.com/blog/batch-stream-a-cheat-sheet)Â (~20 minutes)
			- [Continuous Processing](https://hazelcast.com/glossary/stream-processing/)Â (~10 minutes)
			- [One syntax to rule them all?](https://beam.apache.org/)Â (~2 minutes)
				- Apache Beam is based on theÂ [Dataflow model introduced by Google](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43864.pdf)Â (scientific paper, will take a while to digest)
				- Aims to unify the semantics of batch & streaming processing across engines (Flink, Spark, etc.)
				- You donâ€™t necessarily need streaming, let alone Beam! Many teams simply choose either Spark Structured Streaming or Flink without Beam.
		- Evaluate your own projectâ€™s needs. We will cover streaming in the second half of the course, so more material will be provided then.
	- ## Orchestration Core Concepts[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-3-data-processing/?nav=false#orchestration-core-concepts "Direct link to Orchestration Core Concepts")
		- But how do we make our pipelineÂ **flow**? ðŸŒŠ
			- Data Engineering workflows often involve transforming and transferring data from one place to another.
			- We want to combine data from different locations, and we want to do this in a way that isÂ **reproducible**Â andÂ **scalable**Â when there are updates to the data or to our workflows.
			- Workflows in real-life have multiple steps and stages. We want to be able toÂ **orchestrate**Â these steps and stages andÂ **monitor**Â the progress of our workflows.
			- Sometimes, everything might work fine with just CRON jobs.
			- But other times, you might want to control the state transitions of these steps:
				- e.g. if Step A doesnâ€™t run properly, donâ€™t run Step B because the data could be corrupt, instead run Step C.
				- Once again, the concept ofÂ [Directed Acyclic Graphs (DAGs)](https://en.wikipedia.org/wiki/Directed_acyclic_graph)Â can come to our rescue.
			- **Bonus Content**:Â [Apache Airflow](https://www.youtube.com/watch?v=XD7euLOzKbs&ab_channel=SFPython)Â (32 minute video) is one nice way of setting up DAGs to orchestrate jobs ðŸŒˆ
				- Note: Airflow is primarily designed as a task orchestration tool.
				- You can trigger tasks on the Airflow cluster itself or on remote targets (e.g. AWS Fargate, Databricks, etc.).
				- NOT designed for transferring large amounts of actual data.
				- [Reference Documentation](https://airflow.apache.org/docs/apache-airflow/1.10.1/#beyond-the-horizon)
				- [Play around with Airflow locally](https://github.com/kelseymok/airflow/)Â (very optional!)
	- ## Practical Data Workloads[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-3-data-processing/?nav=false#practical-data-workloads "Direct link to Practical Data Workloads")
		- ![big-data-sword.png](https://data-derp.github.io/assets/images/big-data-sword-f661a3f9269c794183fe4ce3480a796d.png)
		- Weâ€™re here to teach you big data skills, but in reality...
		- ### Single-Node vs. Cluster[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-3-data-processing/?nav=false#single-node-vs-cluster "Direct link to Single-Node vs. Cluster")
			- Not everything is Big Data! You donâ€™t always need Spark!Â [Sometimes Pandas deployed on a single node function/container is just fine!](https://www.indellient.com/blog/a-journey-from-pandas-to-spark-data-frames/).
		- ### Batch vs Streaming[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-3-data-processing/?nav=false#batch-vs-streaming "Direct link to Batch vs Streaming")
			- [Streaming isnâ€™t always the solution](https://www.section.io/engineering-education/batch-processing-vs-stream-processing/)! (Optional reading, ~10 minutes)
		- ### Orchestration options[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-3-data-processing/?nav=false#orchestration-options "Direct link to Orchestration options")
			- Here, it's just useful to know a few of the names but going into detail is not necessary for the course.
			- **DAG-based approaches:**
				- [Apache Airflow](https://airflow.apache.org/)
				- [Databricks Jobs Orchestration](https://databricks.com/blog/2021/07/13/announcement-orchestrating-multiple-tasks-with-databricks-jobs-public-preview.html)
				- [Dagster](https://dagster.io/)
			- **Event-Driven + Declarative**
				- [Databricks Auto Loader](https://databricks.com/discover/demos/delta-lake-data-integration-demo-auto-loader-and-copy-into)
				- [Delta Live Tables](https://databricks.com/discover/demos/delta-live-tables-demo)
			- **Other triggers:**
				- [AWS Lambda](https://aws.amazon.com/lambda/)
				- [Glue Triggers](https://docs.aws.amazon.com/glue/latest/dg/trigger-job.html)
-
- # Data Milky Way: A Brief History (Part 4) - Architecture Reference
  collapsed:: true
	- Data Engineering: The Good Parts (4/4)
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=Mzz4o2xDVzw&ab_channel=PawaritLaosunthara}}
	- ## Typical Data Pipeline
		- ![typical-data-pipeline-b2ce97ec3c25bcdc5bf36b8a5a5fbda0.png](../assets/typical-data-pipeline-b2ce97ec3c25bcdc5bf36b8a5a5fbda0_1718695071423_0.png)
	- ## Open Source Example Architecture
		- ![multi-cloud-cloud-agnostic-architecture-f5cce53e14f4dd3c90e53749c0d1ba9a.png](../assets/multi-cloud-cloud-agnostic-architecture-f5cce53e14f4dd3c90e53749c0d1ba9a_1718695201223_0.png)
		- [Reference](https://towardsdatascience.com/scalable-efficient-big-data-analytics-machine-learning-pipeline-architecture-on-cloud-4d59efc092b5)
	- ## AWS Overview
		- ![aws-overview-8434b2ed7797d6f3438693e4abf93c39.png](../assets/aws-overview-8434b2ed7797d6f3438693e4abf93c39_1718695322924_0.png)
		- [Reference Architecture](https://aws.amazon.com/blogs/architecture/field-notes-how-to-build-an-aws-glue-workflow-using-the-aws-cloud-development-kit/)
		- This example uses AWS Glue. There are other AWS services like EMR that can handle even more complex cases.
	- ## Batch Processing on Azure
		- ![batch-processing-on-azure-fcc0e7ccec79a25820d3782a858145f9.png](../assets/batch-processing-on-azure-fcc0e7ccec79a25820d3782a858145f9_1718695375389_0.png)
		- [Reference](https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/ingest-etl-stream-with-adb)
	- ## Batch & Streaming (Lambda Architecture) on Azure
		- ![batch-streaming-azure-lambda-954f3ce2e50074290e9b507f4ff79563.png](../assets/batch-streaming-azure-lambda-954f3ce2e50074290e9b507f4ff79563_1718695403708_0.png)
		- [Reference](https://docs.microsoft.com/en-us/azure/architecture/example-scenario/dataplate2e/data-platform-end-to-end)
	-
	- ## Lambda vs. Kappa Architecture[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-milky-way-brief-history-part-4-arch-ref/?nav=false#lambda-vs-kappa-architecture "Direct link to Lambda vs. Kappa Architecture")
		- Read about theÂ [differences](https://luminousmen.com/post/modern-big-data-architectures-lambda-kappa).
		- **Lambda**: The two layers (speed layer & batch layer) resemble the two legs of the Î» symbol.
			- ![lambda-44cc93cab966934a57d7181ec1619b94.png](../assets/lambda-44cc93cab966934a57d7181ec1619b94_1718695436777_0.png)
	- **Kappa**: The left stem of the ðš± (kappa symbol) signifies unified input data store and processing logic.
		- ![kappa-4965dcbd5987554830e077cea1ceac1b.png](../assets/kappa-4965dcbd5987554830e077cea1ceac1b_1718695456672_0.png)
	-
	- 1.  **Lambdaæž¶æž„**ï¼š
		- **ç‰¹ç‚¹**ï¼šLambdaæž¶æž„å°†æ•°æ®å¤„ç†æµç¨‹åˆ†ä¸ºæ‰¹å¤„ç†å±‚ï¼ˆBatch Layerï¼‰å’Œé€Ÿåº¦å±‚ï¼ˆSpeed Layerï¼‰ï¼Œç»“åˆæ‰¹å¤„ç†å’Œå®žæ—¶å¤„ç†æ¥å¤„ç†æ•°æ®ã€‚
		- **æ•°æ®å¤„ç†**ï¼šæ‰¹å¤„ç†å±‚è´Ÿè´£å¤„ç†åŽ†å²æ•°æ®ï¼Œç”Ÿæˆæ‰¹å¤„ç†è§†å›¾ï¼ˆBatch Viewsï¼‰ï¼Œé€šå¸¸ä½¿ç”¨ç±»ä¼¼Hadoopçš„æ‰¹å¤„ç†æŠ€æœ¯ï¼ˆä¾‹å¦‚MapReduceï¼‰ã€‚
		- **é€Ÿåº¦å±‚**ï¼šé€Ÿåº¦å±‚å¤„ç†æœ€æ–°çš„æ•°æ®ï¼Œå®žæ—¶è®¡ç®—å‡ºå®žæ—¶è§†å›¾ï¼ˆReal-time Viewsï¼‰ï¼Œé€šå¸¸ä½¿ç”¨æµå¤„ç†æŠ€æœ¯ï¼ˆä¾‹å¦‚Apache Stormã€Apache Flinkï¼‰ã€‚
		- **åˆå¹¶å±‚**ï¼šåˆå¹¶å±‚è´Ÿè´£å°†æ‰¹å¤„ç†è§†å›¾å’Œå®žæ—¶è§†å›¾è¿›è¡Œåˆå¹¶ï¼Œæä¾›ä¸€è‡´çš„æŸ¥è¯¢ç»“æžœã€‚
		- **ä¼˜ç‚¹**ï¼šLambdaæž¶æž„é€‚ç”¨äºŽéœ€è¦å¤„ç†å¤§é‡åŽ†å²æ•°æ®å’Œå®žæ—¶æ•°æ®ï¼Œå¹¶ä¸”èƒ½å¤Ÿå®¹å¿ç¨è®¸å»¶è¿Ÿçš„åº”ç”¨åœºæ™¯ã€‚
	- 2.  **Kappaæž¶æž„**ï¼š
		- **ç‰¹ç‚¹**ï¼šKappaæž¶æž„æå‡ºç®€åŒ–æž¶æž„ï¼Œåªä½¿ç”¨æµå¤„ç†æ¥å¤„ç†æ‰€æœ‰æ•°æ®ã€‚å®ƒæ¶ˆé™¤äº†Lambdaæž¶æž„ä¸­çš„æ‰¹å¤„ç†å±‚ï¼Œæ‰€æœ‰æ•°æ®å¤„ç†éƒ½é€šè¿‡æµå¤„ç†å®žæ—¶å®Œæˆã€‚
		- **æ•°æ®å¤„ç†**ï¼šKappaæž¶æž„ä¸­æ•°æ®ä»…é€šè¿‡æµå¤„ç†æŠ€æœ¯ï¼ˆä¾‹å¦‚Apache Kafkaã€Apache Flinkï¼‰è¿›è¡Œå¤„ç†ï¼Œä¸å­˜åœ¨æ‰¹å¤„ç†è¿‡ç¨‹ã€‚
		- **ä¼˜ç‚¹**ï¼šKappaæž¶æž„ç®€åŒ–äº†æž¶æž„ï¼Œå‡å°‘äº†ç³»ç»Ÿçš„å¤æ‚æ€§å’Œç»´æŠ¤æˆæœ¬ï¼ŒåŒæ—¶å®žçŽ°äº†æ›´ä½Žçš„å»¶è¿Ÿå’Œæ›´ç®€å•çš„æ•°æ®ä¸€è‡´æ€§ã€‚
	- 3.  **ä¸»è¦åŒºåˆ«**ï¼š
		- **å¤„ç†æ–¹å¼**ï¼šLambdaæž¶æž„åŒæ—¶ä½¿ç”¨æ‰¹å¤„ç†å’Œæµå¤„ç†æ¥å¤„ç†æ•°æ®ï¼Œè€ŒKappaæž¶æž„åªä½¿ç”¨æµå¤„ç†ã€‚
		- **å¤æ‚æ€§**ï¼šLambdaæž¶æž„ç›¸å¯¹äºŽKappaæž¶æž„æ¥è¯´æ›´å¤æ‚ï¼Œå› ä¸ºéœ€è¦ç®¡ç†ä¸¤ä¸ªä¸åŒçš„æ•°æ®å¤„ç†æµç¨‹ï¼ˆæ‰¹å¤„ç†å’Œæµå¤„ç†ï¼‰ã€‚
		- **å»¶è¿Ÿ**ï¼šKappaæž¶æž„é€šå¸¸å…·æœ‰è¾ƒä½Žçš„å»¶è¿Ÿï¼Œå› ä¸ºæ‰€æœ‰æ•°æ®å¤„ç†éƒ½æ˜¯å®žæ—¶è¿›è¡Œçš„ï¼Œè€ŒLambdaæž¶æž„ä¸­çš„æ‰¹å¤„ç†éƒ¨åˆ†å¯èƒ½ä¼šå¼•å…¥è¾ƒå¤§çš„å»¶è¿Ÿã€‚
-
- # Data Serialisation & Data Formats
  collapsed:: true
	- ## Wait...whatâ€™s data serialisation again?[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-formats/?nav=false#waitwhats-data-serialisation-again "Direct link to Wait...whatâ€™s data serialisation again?")
	  collapsed:: true
		- (extracted fromÂ [Devopedia](https://devopedia.org/data-serialization))
			- Data serialisation is the process of converting data objects present in complex data structures into a byte stream for storage, transfer and distribution purposes on physical devices.
			- Computer systems mayÂ **vary**Â in their hardware architecture, OS, addressing mechanisms. Internal representations of data also vary accordingly in every environment/language. Storing and exchanging data between such varying environments requires aÂ **platform-and-language-neutral data format**Â that all systems understand.
			- Once the serialized data is transmitted from the source machine to the destination machine, the reverse process of creating objects from the byte sequence calledÂ **deserialisation**Â is carried out. Reconstructed objects are clones of the original object.
			- Choice of data serialisation format for an application depends on factors such as data complexity, need for human readability, speed and storage space constraints.
			- ![data-serialisation-6be2b2b59e9b5d74196604a702d791b8.png](../assets/data-serialisation-6be2b2b59e9b5d74196604a702d791b8_1718695821922_0.png)
	- ## Data Formats[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-formats/?nav=false#data-formats "Direct link to Data Formats")
		- Data Serialisation and Data Formats go hand in hand because it describes how data is stored and then retrieved. Based on these patterns on storage and access, we might choose a specific file format to optimise those processes. There are a variety of file formats common to Data Engineering use cases - the classics include CSV, JSON, Avro, and Parquet (among others). There are other more modern data formats which we'll get to a little later.
		- A few important points to note before we dive into individual formats:
			- Data on the hard disks is saved in blocks and gets loaded one at a time in memory.
				- ç¡¬ç›˜ä¸Šçš„æ•°æ®ä»¥å—çš„å½¢å¼ä¿å­˜ï¼Œå¹¶é€å—åŠ è½½åˆ°å†…å­˜ä¸­ã€‚
			- Reading unnecessary, fragmented and random data is expensive therefore sequential reads / writes are recommended.
				- è¯»å–ä¸å¿…è¦ã€ç¢Žç‰‡åŒ–å’Œéšæœºçš„æ•°æ®ä»£ä»·é«˜ï¼Œå› æ­¤å»ºè®®è¿›è¡Œé¡ºåºè¯»å–/å†™å…¥ã€‚
			- File formats determine the way that data is stored on the disk and is of different types for e.g Unstructured (Text, CSV, TSV etc), Semi-structured (jSON, XML etc) and Structured (Avro, Parquet etc).
				- æ–‡ä»¶æ ¼å¼å†³å®šäº†æ•°æ®åœ¨ç£ç›˜ä¸Šçš„å­˜å‚¨æ–¹å¼ï¼ŒåŒ…æ‹¬éžç»“æž„åŒ–ï¼ˆæ–‡æœ¬ã€CSVã€TSVç­‰ï¼‰ã€åŠç»“æž„åŒ–ï¼ˆJSONã€XMLç­‰ï¼‰å’Œç»“æž„åŒ–ï¼ˆAvroã€Parquetç­‰ï¼‰ç­‰ä¸åŒç±»åž‹ã€‚
			- File formats stores data in a row oriented, column oriented or a hybrid format. Irrespective of the data orientation, the data is always arranged on the disk in sequential manner due the benefit of faster reads as mentioned above.
				- æ–‡ä»¶æ ¼å¼å¯ä»¥æŒ‰è¡Œå‘ã€åˆ—å‘æˆ–æ··åˆæ–¹å¼å­˜å‚¨æ•°æ®ã€‚æ— è®ºæ•°æ®çš„æ–¹å‘å¦‚ä½•ï¼Œç”±äºŽä¸Šè¿°å¿«é€Ÿè¯»å–çš„å¥½å¤„ï¼Œæ•°æ®å§‹ç»ˆä»¥é¡ºåºæ–¹å¼æŽ’åˆ—åœ¨ç£ç›˜ä¸Šã€‚
			- Row-wise formats are best for write-heavy operations (OLTP workflows), Columnar are best for read-heavy (OLAP workflows) and Hybrid Formats try to provide the best of both worlds.
				- æŒ‰è¡Œå­˜å‚¨æ ¼å¼æœ€é€‚åˆå†™å…¥å¯†é›†åž‹æ“ä½œï¼ˆOLTPå·¥ä½œæµï¼‰ï¼ŒæŒ‰åˆ—å­˜å‚¨æœ€é€‚åˆè¯»å–å¯†é›†åž‹æ“ä½œï¼ˆOLAPå·¥ä½œæµï¼‰ï¼Œæ··åˆæ ¼å¼è¯•å›¾æä¾›ä¸¤è€…çš„æœ€ä½³ç»„åˆã€‚
		- Some helpful vocabulary:
			- **Splittable**Â = take one file and split it into multiple chunks (partitions) to allow for concurrent processing
				- å°†ä¸€ä¸ªæ–‡ä»¶æ‹†åˆ†æˆå¤šä¸ªå—ï¼ˆåˆ†åŒºï¼‰ï¼Œä»¥å…è®¸å¹¶å‘å¤„ç†
			- **Compressibility**Â = encoding a file using fewer bits than the original representation
				- ä½¿ç”¨æ¯”åŽŸå§‹è¡¨ç¤ºæ›´å°‘çš„ä½å¯¹æ–‡ä»¶è¿›è¡Œç¼–ç 
			- **Self-describing**Â = file contains both data and metadata (about the data, like the schema)
				- æ–‡ä»¶åŒ…å«æ•°æ®å’Œå…³äºŽæ•°æ®çš„å…ƒæ•°æ®ï¼ˆå¦‚æ¨¡å¼ï¼‰
			- **Schema evolution**Â = file can handle new columns and backwards-compatibility
				- æ–‡ä»¶å¯ä»¥å¤„ç†æ–°åˆ—å’Œå‘åŽå…¼å®¹
		- ### CSV[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-formats/?nav=false#csv "Direct link to CSV")
			- Row-based and human-readable
			- Compressible and splittable
			- Faster writes but slower reads
			- Flexible (as in, data can be easily changed, but can also be corrupted) but does not support schema evolution
			- Best suited for smaller data sets which don't need any query time optimizations
		- ### JSON[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-formats/?nav=false#json "Direct link to JSON")
			- Row-based and human-readable
			- Compressible and splittable
			- Faster writes but slower reads
			- Self-describing
			- Support complex data type like arrays and nested values
			- Schema Evolution is not supported though column names are embedded inside the datasets
			- Best suited for if the dataset has highly nested values which, like CSV, does not need query optimizations
			- Suited for smaller data sets if used as a data exchange format
		- ### Avro[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-formats/?nav=false#avro "Direct link to Avro")
			- Row-based but not human-readable
			- is itself a serialization format so it can be used for data exchange across the wire across different OS
			- Self describing with support for Schema Evolution
			- Binary format with Schema stored inside the file in JSON format
			- Optimised for write-intensive applications
			- Compressible and splittable
			- Supports rich data structures like arrays and enumerated types
			- Best suited for both realtime and batch workflows where fast data writes and transfer, schema validation and evolution is required.
			- ![avro-recap-f7e91e9216ec0deaac82a10005d57d28.png](../assets/avro-recap-f7e91e9216ec0deaac82a10005d57d28_1718695848086_0.png)
		- ### Parquet[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-formats/?nav=false#parquet "Direct link to Parquet")
			- Columnar-based and not human-readable
			- Self-describing
			- Hybrid format where rows are grouped by row groups and then columns are partitioned
			- Binary format like Avro with Schema stored inside the file in JSON format but does not support Schema Evolution
			- Optimised for read intensive applications but performs well for write operations too due to its hybrid nature
			- Compressible and splittable
			- Support rich data structures like arrays and enumerated types, again like Avro
			- Best suited for analytics workflow where fast query performance is needed
			- ![parquet-columnar-storage-c1166682d766e4dd8de15d50b22a7e7f.png](../assets/parquet-columnar-storage-c1166682d766e4dd8de15d50b22a7e7f_1718695868642_0.png)
			- ![parquet-columnar-storage-for-the-people-7eb0d687dcb9a723df3e87cf7e85453c.png](../assets/parquet-columnar-storage-for-the-people-7eb0d687dcb9a723df3e87cf7e85453c_1718695876641_0.png)
			- ![parquet-access-only-data-you-need-9faae93d189f90df94db2961a37b5ff2.png](../assets/parquet-access-only-data-you-need-9faae93d189f90df94db2961a37b5ff2_1718695882724_0.png)
	- ### Additional Resources (optional)[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-formats/?nav=false#additional-resources-optional "Direct link to Additional Resources (optional)")
		- [Big Data File Formats](https://luminousmen.com/post/big-data-file-formats)
		- [Comparing the formats](https://www.datanami.com/2018/05/16/big-data-file-formats-demystified/#:~:text=The%20biggest%20difference%20between%20ORC,in%20a%20row%2Dbased%20format.&text=While%20column%2Doriented%20stores%20like,might%20be%20the%20better%20choice.)
		- [Big Data File Showdown: Avro vs Parquet](https://www.confessionsofadataguy.com/big-data-file-showdown-avro-vs-parquet-with-python/)
		- Avro is aÂ [popular choice for streaming](https://www.confluent.io/blog/avro-kafka-data/)Â and persisting streaming data into data lakes (e.g.Â [Azure Event Hubs Capture](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview#exploring-the-captured-files-and-working-with-avro)).
		- The Apache Spark File Format Ecosystem
			- {{video https://www.youtube.com/watch?v=auNAzC3AU18&t=153s&ab_channel=Databricks}}
		- The Parquet Format and Performance Optimization Opportunities Boudewijn Braams (Databricks)
			- {{video https://www.youtube.com/watch?v=1j8SdS7s_NY&ab_channel=Databricks}}
	- ## Check Your Learning![â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-formats/?nav=false#check-your-learning "Direct link to Check Your Learning!")
		- Unlike MapReduce vs Spark, thereâ€™s no clear winner. Thereâ€™s always still a time and place for each of these formats!
		- |  | CSV | JSON | Parquet | Avro |
		  | --- | --- | --- | --- | --- |
		  | Compressibility | âœ… | âœ… | âœ… | âœ… |
		  | Human Readability | âœ… | âœ… |  |  |
		  | Schema Evolution |  |  |  | âœ… |
		  | Row or Columnar Storage |  |  | âœ… | âœ… |
	- ## Bonus: Delta Lake[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/data-formats/?nav=false#bonus-delta-lake "Direct link to Bonus: Delta Lake")
		- We'll cover Delta Lake in detail in the "Making Big Data Work" section, but let's define it here so you can keep an ear out for it. Delta Lake is anÂ **open source**Â storage layer. It is similar to Parquet (some people refer to it as Parquet Plus) but it provides ACID transactions, time travel, the ability to remove data across various versions (vaccuuming), and you can stream and batch at the same time.
-
- # Apache Spark Primer
  collapsed:: true
	- ## Introduction to Spark[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/apache-spark-primer/?nav=false#introduction-to-spark "Direct link to Introduction to Spark")
		- **A Must-Read:**Â [Slides from Brooke Wenig](https://brookewenig.com/SparkOverview.html#/)
		- Apache Spark: Why use Spark?
			- {{video https://www.youtube.com/watch?v=35Mjaa1YWTk&ab_channel=SundogEducationwithFrankKane}}
		- Important comment upon Frank Kaneâ€™s video
			- Scala is only faster than Python if youâ€™re writing a lot of custom UDFs or data structures with RDDs. If youâ€™re using built-in Spark functions, then performance isÂ **identical**.
			- Most modern Spark users are shifting towards a Python codebase to take advantage of modern data science and machine learning tools - see next slides for empirical evidence ðŸ˜‰.
	- Takeaways
		- What is the difference between Transformations (lazy evaluation) and Actions?
		- What is the difference between Driver and Worker nodes?
	- ## Programming language popularity[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/apache-spark-primer/?nav=false#programming-language-popularity "Direct link to Programming language popularity")
		- Python has massively overtaken Scala in popularity for Spark.
			- ![spark-usage-2013-db48e6a4dd355e30073a86ac932c4ded.png](../assets/spark-usage-2013-db48e6a4dd355e30073a86ac932c4ded_1718697692205_0.png)
			- ![spark-usage-2020-993446646e8f0d0ca828edf6ba1d18fb.png](../assets/spark-usage-2020-993446646e8f0d0ca828edf6ba1d18fb_1718697701681_0.png)
		- For the exercises of this course we will use Python. The APIs we will be using are:
			- Spark DataFrames
			- Pandas
		- **Bonus Content:**Â [Spark SQL Programming Guide](http://spark.apache.org/docs/3.1.1/sql-getting-started.html)
	- ## Bonus Content: Data + AI Summit 2020[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/apache-spark-primer/?nav=false#bonus-content-data--ai-summit-2020 "Direct link to Bonus Content: Data + AI Summit 2020")
		- One of the keynote presentations from the Chief Architect of Databricks
			- Project Zen: Making Spark Pythonic | Reynold Xin | Keynote Data + AI Summit EU 2020
			  collapsed:: true
				- {{video https://www.youtube.com/watch?v=-vJLTEOdLvA&ab_channel=Databricks}}
		- Heading towards taking advantage of idiomatic Python with type hints
		- Improving Python debugging is on the Databricks roadmap
		- Thereâ€™s no denying the rich ecosystem of libraries, especially for advanced analytics & ML
-
- # Look into the Past (Bonus)
  collapsed:: true
	- Hadoop/MapReduce are in the past, but it is still important to understand what they are...
	- MicroNugget: What is MapReduce?
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=s8EPQpgpWVE&ab_channel=CBTNuggets}}
	- ...and why we have moved on
	- What Happened to Hadoop?
	- {{video https://www.youtube.com/watch?v=e0Kjf55eyog&ab_channel=DatabaseTrendsandApplications}}
-
- # Quiz
  collapsed:: true
	- What are some key differences between OLAP and OLTP systems?
		- > **OLTP**
		  Frequent updates, transactional behaviour/requirements
		  Query priorities:
		  low-latency, up-to-date, consistent data for standard business operations
		  **OLAP**
		  Oriented towards analysis, modelling, reporting, business intelligence
		  Query Priorities:
		  asking big questions against big data, supporting flexible queries/analyses
		  efficiently processing millions of rows and returning thousands/millions of rows
	- Why did people start moving from traditional databases and data warehouses to HDFS for Big Data Analytics in the mid 2000s?
		- Traditional relational databases and data warehouses relied on vertical scaling which required expensive/specialized hardware and was hard to plan for. HDFS and MapReduce allowed for horizontal scalability with cheap commodity hardware.
	- And subsequently, why did people move from HDFS to Object Storage for Big Data Analytics?
		- Cloud-based Object Storage is super scalable and cheap, far more than HDFS (which is only often used in on-prem environments these days). It was also great for storing all sorts of file formats (unstructured text, images, video, etc.). Most modern cloud-based Data Lakes are built-on Object Storage technologies. Modern query engines such as Apache Spark, Presto, Dremio, etc. can efficiently scan data laying on object stores. To summarize: object storage + (on-demand) query engines = fully decoupled storage and compute (great for scalability, elasticity, cost)
	- Name some object storage offerings from the 3 major cloud providers
		- AWS
			- Amazon S3 (general purpose)
			- Amazon S3 Glacier (for archiving)
		- Microsoft Azure
			- Azure Blob Storage (general purpose)
			- Azure Data Lake Storage Gen2 (has a file systems hierarchy, great for large structured and semi-structured datasets)
		- GCP
			- Google Cloud Storage or GCS (general purpose)
	- Assign either the word OLTP or OLAP to the following technologies
		- Operational Databases
			- OLTP
		- MongoDB or Cassandra
			- Depends (youâ€™ll find use-cases online for either)
		- Data Warehouses (e.g. Oracle, Snowflake, Redshift)
			- OLAP
		- Apache Spark
			- OLAP
		- Presto/Athena
			- OLAP
-
- # Exercise: Gentle Introduction to Spark
  collapsed:: true
	- Follow instructionsÂ [here](https://github.com/data-derp/small-exercises/tree/master/gentle-introduction-to-spark)Â to get started
	- collapsed:: true
	  ```python
	  import pyspark.sql.functions as F
	  
	  # let's add a new column: instead of 1, 2, 3, 4, 5 let's make it a percentage between 0 and 1
	  # e.g. 4/5 => 80%
	  brand_new_column = F.col("rating_percentage") / 2 # with Column expressions, you don't need a column to even exist yet!
	  
	  df = df.withColumn("rating_percentage", (F.col("rating") / F.lit(5)) * F.lit(100))
	  display(df.withColumn("half_rating_percentage", brand_new_column))
	  ```
		- è¿™æ®µä»£ç å±•ç¤ºäº†å¦‚ä½•åœ¨Databricksæˆ–ç±»ä¼¼çš„Apache SparkçŽ¯å¢ƒä¸­ä½¿ç”¨PySparkåº“æ¥å¤„ç†æ•°æ®æ¡†ï¼ˆDataFrameï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®ƒæ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡è®¡ç®—å’Œæ·»åŠ æ–°çš„åˆ—æ¥è½¬æ¢æ•°æ®ã€‚è®©æˆ‘ä»¬é€æ­¥è§£é‡Šæ¯ä¸€éƒ¨åˆ†çš„å«ä¹‰ï¼š
		- ### ä»£ç è§£é‡Šï¼š
		  
		  1. **å¯¼å…¥å¿…è¦çš„åº“**ï¼š
			- å‡è®¾ `F` ä»£è¡¨çš„æ˜¯ `pyspark.sql.functions`ï¼Œä¸€ä¸ªåŒ…å«è®¸å¤šå†…ç½®Spark SQLå‡½æ•°çš„æ¨¡å—ã€‚
			  
			  2. **åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ—è¡¨è¾¾å¼ `brand_new_column`**ï¼š
			  ```python
			  brand_new_column = F.col("rating_percentage") / 2
			  ```
			- è¿™é‡Œ `F.col("rating_percentage")` æ˜¯æŒ‡ä¸€ä¸ªåä¸º `rating_percentage` çš„åˆ—ã€‚
			- å°† `rating_percentage` åˆ—çš„å€¼é™¤ä»¥ 2ã€‚
			- é‡è¦çš„æ˜¯ï¼Œè¿™é‡Œ `rating_percentage` åˆ—è¿˜ä¸å­˜åœ¨ï¼Œå®ƒä¼šåœ¨åŽç»­æ­¥éª¤ä¸­åˆ›å»ºã€‚
			  
			  3. **åˆ›å»ºå¹¶æ·»åŠ æ–°åˆ— `rating_percentage` åˆ°æ•°æ®æ¡† `df`**ï¼š
			  ```python
			  df = df.withColumn("rating_percentage", (F.col("rating") / F.lit(5)) * F.lit(100))
			  ```
			- ä½¿ç”¨ `withColumn` æ–¹æ³•å‘ `df` æ•°æ®æ¡†æ·»åŠ ä¸€ä¸ªæ–°åˆ—ï¼Œåˆ—åä¸º `rating_percentage`ã€‚
			- `F.col("rating")` èŽ·å– `rating` åˆ—çš„å€¼ã€‚
			- `F.lit(5)` åˆ›å»ºä¸€ä¸ªå¸¸é‡å€¼ 5ã€‚
			- `F.col("rating") / F.lit(5)` å°† `rating` åˆ—çš„å€¼é™¤ä»¥ 5ï¼Œå¾—åˆ°ä¸€ä¸ª0åˆ°1ä¹‹é—´çš„ç™¾åˆ†æ¯”ã€‚
			- `* F.lit(100)` å°†å‰é¢çš„ç»“æžœä¹˜ä»¥ 100ï¼Œè½¬æ¢æˆç™¾åˆ†æ¯”å½¢å¼ã€‚
			- æœ€ç»ˆçš„è®¡ç®— `(F.col("rating") / F.lit(5)) * F.lit(100)` ç”Ÿæˆä¸€ä¸ªä»Ž 0 åˆ° 100 çš„ç™¾åˆ†æ¯”ã€‚
			  
			  4. **æ˜¾ç¤ºæ•°æ®æ¡†å¹¶æ·»åŠ å¦ä¸€åˆ— `half_rating_percentage`**ï¼š
			  ```python
			  display(df.withColumn("half_rating_percentage", brand_new_column))
			  ```
			- ä½¿ç”¨ `withColumn` æ–¹æ³•å†æ¬¡å‘ `df` æ•°æ®æ¡†æ·»åŠ ä¸€ä¸ªæ–°åˆ—ï¼Œåˆ—åä¸º `half_rating_percentage`ï¼Œå…¶å€¼æ˜¯ä¹‹å‰å®šä¹‰çš„ `brand_new_column` è¡¨è¾¾å¼çš„ç»“æžœï¼Œå³ `rating_percentage` åˆ—çš„å€¼é™¤ä»¥ 2ã€‚
			- `display` æ˜¯Databricksä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºŽåœ¨ç¬”è®°æœ¬çŽ¯å¢ƒä¸­å±•ç¤ºæ•°æ®æ¡†çš„å†…å®¹ã€‚
		- ### æ€»ç»“
		  æ•´ä¸ªè¿‡ç¨‹å¦‚ä¸‹ï¼š
		- å…ˆå®šä¹‰ä¸€ä¸ªæ–°çš„åˆ—è¡¨è¾¾å¼ `brand_new_column`ï¼Œè¡¨ç¤º `rating_percentage` åˆ—çš„å€¼é™¤ä»¥2ã€‚
		- ç„¶åŽå‘æ•°æ®æ¡† `df` æ·»åŠ ä¸€ä¸ªæ–°çš„åˆ— `rating_percentage`ï¼Œå…¶å€¼æ˜¯ `rating` åˆ—çš„ç™¾åˆ†æ¯”è¡¨ç¤ºã€‚
		- æœ€åŽï¼Œå±•ç¤ºæ·»åŠ äº† `rating_percentage` å’Œ `half_rating_percentage` ä¸¤åˆ—çš„æ•°æ®æ¡†ã€‚
		- ### ç¤ºä¾‹
		  å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®æ¡† `df` å¦‚ä¸‹ï¼š
		  ```plaintext
		  +-------+
		  | rating|
		  +-------+
		  |      1|
		  |      2|
		  |      3|
		  |      4|
		  |      5|
		  +-------+
		  ```
		  
		  æ‰§è¡Œä¸Šè¿°ä»£ç åŽï¼Œæ•°æ®æ¡† `df` å°†å˜æˆï¼š
		  ```plaintext
		  +-------+----------------+----------------------+
		  | rating|rating_percentage|half_rating_percentage|
		  +-------+----------------+----------------------+
		  |      1|             20.0|                  10.0|
		  |      2|             40.0|                  20.0|
		  |      3|             60.0|                  30.0|
		  |      4|             80.0|                  40.0|
		  |      5|            100.0|                  50.0|
		  +-------+----------------+----------------------+
		  ```
		  
		  å¸Œæœ›è¿™ä¸ªè§£é‡Šèƒ½å¸®åŠ©ä½ ç†è§£ä»£ç çš„æ„ä¹‰å’Œæ‰§è¡Œè¿‡ç¨‹ã€‚å¦‚æžœæœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶æé—®ã€‚
	- ### Can I make my OWN DataFrames??
		- collapsed:: true
		  ```python
		  import pandas as pd
		  from datetime import datetime 
		  
		  pandas_df = pd.DataFrame({"x": [1, 2, 3, 4], "y": ["A", "B", "C", "D"]})
		  pandas_df["t"] = [datetime.now()] * len(pandas_df)
		  pandas_df
		  ```
			- ![image.png](../assets/image_1718702097326_0.png)
		- collapsed:: true
		  ```python
		  spark_df = spark.createDataFrame(pandas_df)
		  spark_df = spark_df.withColumn("z", F.col("t").cast(LongType())) # convert to Epoch seconds
		  spark_df = spark_df.withColumn("t_recreated", F.col("z").cast(TimestampType()))
		  ```
			- ![image.png](../assets/image_1718702111256_0.png)
		- collapsed:: true
		  ```python
		  spark_df.collect()
		  ```
			- ```
			  [Row(x=1, y='A', t=datetime.datetime(2024, 6, 18, 9, 10, 39, 841747), z=1718701839, t_recreated=datetime.datetime(2024, 6, 18, 9, 10, 39)),
			   Row(x=2, y='B', t=datetime.datetime(2024, 6, 18, 9, 10, 39, 841747), z=1718701839, t_recreated=datetime.datetime(2024, 6, 18, 9, 10, 39)),
			   Row(x=3, y='C', t=datetime.datetime(2024, 6, 18, 9, 10, 39, 841747), z=1718701839, t_recreated=datetime.datetime(2024, 6, 18, 9, 10, 39)),
			   Row(x=4, y='D', t=datetime.datetime(2024, 6, 18, 9, 10, 39, 841747), z=1718701839, t_recreated=datetime.datetime(2024, 6, 18, 9, 10, 39))]
			  ```
		- collapsed:: true
		  ```python
		  display(spark_df)
		  ```
			- ![image.png](../assets/image_1718702164506_0.png)
-
- # Vanilla Spark on your Local Machine (Bonus)
	- ## Running Spark locally: Spark Shell & UI[â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/demo-vanilla-spark/#running-spark-locally-spark-shell--ui "Direct link to Running Spark locally: Spark Shell & UI")
		- Basics of Spark Shell
		- Basics of Spark UI
			- Can be seen through Spark Shells
			- Also in the Databricks Notebooks
		- Spark Shell and UI
		  collapsed:: true
			- {{video https://www.youtube.com/watch?v=y2M6nYhpPPg&ab_channel=DataDerp}}
		- To practice the concepts and examples that are demonstrated in the above video, specific versions of Apache Spark and Java Standard Edition are required to be installed on your machines. Follow the instructionsÂ [here](https://github.com/data-derp/exercise-vanilla-spark/tree/main#readme)Â to do so.
	- ## Spark Ecosystem, Spark Session (Spark Object)
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=2KlydmMU9ko&ab_channel=DataDerp}}
	- ## Spark Program Structure
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=T1pD6lwssXc&ab_channel=DataDerp}}
	- ## Spark Applications and Jobs
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=byMySPRrPB4&ab_channel=DataDerp}}
	- ## Spark Input Partitions, Stages, and DAGs
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=zOZtmc-yOnw&ab_channel=DataDerp}}
	- ## Spark Tasks and Operations
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=dSuNVxwl9FU&ab_channel=DataDerp}}
	- ## Do it yourself![â€‹](https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/demo-vanilla-spark/#do-it-yourself "Direct link to Do it yourself!")
		- Starting from the video below, a Git repo is used to demonstrate the examples in Pycharm. If you would like to follow along on your local machine, set up the repo in Pycharm by following the instructionsÂ [here](https://github.com/data-derp/exercise-vanilla-spark/tree/main#repo-set-up-in-pycharm).
	- ## Spark Read data from files - Part 1
	  collapsed:: true
		- {{video https://www.youtube.com/watch?v=koW-Wq7WULs&ab_channel=DataDerp}}
	- ## Spark Read data from files - Part 2
		- {{video https://youtu.be/KuMf9wevWcI}}
	- ## Spark groupBy and Shuffle Partitions - Part 1
		- {{video https://youtu.be/XKK4DIIUkeo}}
	- ## Spark groupBy and Shuffle Partitions - Part 2
		- {{video https://youtu.be/kamn0HUnosw}}
	- ## Spark Repartitions - Part 1
		- {{video https://youtu.be/eJlFlVbRIPI}}
	- ## Spark Repartitions - Part 2
		- {{video https://youtu.be/6u8k4gfDYmY}}
	- ## Spark Program on the cluster - detailed explanation
		- {{video https://youtu.be/UjAonolAYgU}}
	- ## Spark PartitionBy
		- {{video https://youtu.be/YZEel_twezA}}
	- ## Spark BucketBy
		- {{video https://youtu.be/LT8TUMbZvoo}}
	- ## Spark Submit - Part 1
		- {{video https://youtu.be/52BVRpcv85A}}
	- ## Spark Submit - Part 2
		- {{video https://youtu.be/W-_WtKQ4kL8}}
	- ## Spark Submit - Part 3
		- {{video https://youtu.be/iA0V-zZ38QQ}}
	- ## Spark Structured Streaming - Introduction
		- {{video https://youtu.be/4X5rRI4M_II}}
	- ## Spark Structured Streaming - Example 1
		- {{video https://youtu.be/HdoBE2EhvIQ}}
	- ## Spark Structured Streaming - Example 2
		- {{video https://youtu.be/J2s8k40X2GA}}
	- ## Spark Structured Streaming - Example 3
		- {{video https://youtu.be/F5XzssAX8UI}}
	- ## Spark Caching - Introduction
		- {{video https://youtu.be/Lecq2uhQrTM}}
	- ## Spark Caching - Example 1
		- {{video https://youtu.be/Qjvl0j3Bcns}}
	- ## Spark Caching - Example 2
		- {{video https://youtu.be/huD72RkWA24}}
	- ## Spark Caching - Example 3
		- {{video https://youtu.be/7A6cOKoadME}}
	- ## Summary
		- {{video https://youtu.be/JiZPOYYBZ0U}}
-
-